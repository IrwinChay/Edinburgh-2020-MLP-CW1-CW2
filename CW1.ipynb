{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 1\n",
    "\n",
    "This notebook is intended to be used as a starting point for your experiments. The instructions can be found in the instructions file located under spec/coursework1.pdf. The methods provided here are just helper functions. If you want more complex graphs such as side by side comparisons of different experiments you should learn more about matplotlib and implement them. Before each experiment remember to re-initialize neural network weights and reset the data providers so you get a properly initialized experiment. For each experiment try to keep most hyperparameters the same except the one under investigation so you can understand what the effects of each are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer, ReluLayer\n",
    "from mlp.layers import LeakyReluLayer, RandomReluLayer, ParametricReluLayer, ExponentialLinearUnitLayer\n",
    "from mlp.errors import CrossEntropySoftmaxError\n",
    "from mlp.models import MultipleLayerModel\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "from mlp.learning_rules import AdamLearningRule, GradientDescentLearningRule\n",
    "from mlp.optimisers import Optimiser\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def get_file_name(prefix, learning_rate, hidden_layers_num, hidden_dim, num_epochs, \n",
    "                  activation=None, optimizer=None, index=None, alpha=None, \n",
    "                  lower=None, upper=None, suffix=None):\n",
    "    \n",
    "    file_name = prefix \n",
    "    \n",
    "    if activation is not None:\n",
    "        file_name = file_name + \"_\" + str(activation)\n",
    "    if optimizer is not None:\n",
    "        file_name = file_name + \"_\" + str(optimizer)\n",
    "    \n",
    "    file_name = file_name + \"_\" + str(learning_rate) + \"_\" + str(hidden_layers_num) + \"_\" + str(hidden_dim) + \"_\" + str(num_epochs)\n",
    "    \n",
    "    if index is not None:\n",
    "        file_name = file_name + \"_index\" + str(index)\n",
    "    if alpha is not None:\n",
    "        file_name = file_name + \"_alpha\" + str(alpha)\n",
    "    if lower is not None:    \n",
    "        file_name = file_name + \"_lower\" + str(lower)\n",
    "    if upper is not None:    \n",
    "        file_name = file_name + \"_upper\" + str(upper)\n",
    "        \n",
    "    if suffix is not None:\n",
    "        file_name = file_name + str(suffix)\n",
    "    \n",
    "    return file_name\n",
    "    \n",
    "def save_figures(fig_1, fig_2, learning_rate, num_hidden_layers, hidden_units, num_epochs, \n",
    "                 activation=None, optimizer=None, index=None, alpha=None, \n",
    "                 lower=None, upper=None, suffix=None):\n",
    "    fig_1.tight_layout()\n",
    "    fig_2.tight_layout()\n",
    "    fig_1_type = \"figures/error\"\n",
    "    fig_2_type = \"figures/acc\"\n",
    "    fig_1_name = get_file_name(fig_1_type, learning_rate, num_hidden_layers, hidden_units, num_epochs, \n",
    "                               activation=activation, optimizer=optimizer, index=index, alpha=alpha, \n",
    "                               lower=lower, upper=upper, suffix=suffix)\n",
    "    fig_2_name = get_file_name(fig_2_type, learning_rate, num_hidden_layers, hidden_units, num_epochs, \n",
    "                               activation=activation, optimizer=optimizer, index=index, alpha=alpha, \n",
    "                               lower=lower, upper=upper, suffix=suffix)\n",
    "    \n",
    "    fig_1.savefig(fig_1_name)\n",
    "    fig_2.savefig(fig_2_name)\n",
    "\n",
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, learning_rate, train_data, valid_data, num_hidden_layers,\n",
    "        hidden_units, num_epochs, stats_interval, \n",
    "        notebook=True, test_data=None, activation=None, optimizer=None, alpha=None, \n",
    "        lower=None, upper=None, index=None):\n",
    "\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, \n",
    "        notebook=notebook, test_dataset=test_data)\n",
    "\n",
    "    stats, keys, run_time, test_stats, model = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "\n",
    "\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    \n",
    "\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    plt.show()\n",
    "    save_figures(fig_1, fig_2, learning_rate, num_hidden_layers, hidden_units, num_epochs, \n",
    "                 activation=activation, optimizer=optimizer, index=index, alpha=alpha, \n",
    "                 lower=lower, upper=upper, suffix=\".pdf\")\n",
    "    \n",
    "    return stats, keys, run_time, fig_1, ax_1, fig_2, ax_2, test_stats, model\n",
    "\n",
    "def test_model(\n",
    "        model, error, learning_rule, train_data, valid_data, test_data, num_epochs, stats_interval, \n",
    "        notebook=True):\n",
    "\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, \n",
    "        notebook=notebook, test_dataset=test_data)\n",
    "\n",
    "\n",
    "    test_results = optimiser.test(test_data)\n",
    "\n",
    "    return rest_results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_params(model_params, file_name):\n",
    "    \"\"\"\n",
    "    Saves model weights into a txt file\n",
    "    \"\"\"\n",
    "    with open(file_name, 'w') as f:\n",
    "        for item in model_params:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "            \n",
    "def save_results(val_results, test_results, learning_rate, hidden_layers_num, hidden_dim, num_epochs, activation=None,\n",
    "                optimizer=None, index=None, alpha=None, lower=None, upper=None):\n",
    "    file = open(\"results.txt\", \"a\")\n",
    "    if index is not None:\n",
    "        file.write(\"index: \" + str(index) + \"\\n\")\n",
    "    if lower is not None:\n",
    "        file.write(\"lower: \" + str(lower) + \"\\n\")\n",
    "    if upper is not None:\n",
    "        file.write(\"upper: \" + str(upper) + \"\\n\")\n",
    "    if alpha is not None:\n",
    "        file.write(\"alpha: \" + str(alpha) + \"\\n\")\n",
    "    if optimizer is not None:\n",
    "        file.write(\"optimizer: \" + str(optimizer) + \"\\n\")\n",
    "    if activation is not None:\n",
    "        file.write(\"activation: \" + str(activation) + \"\\n\")\n",
    "    file.write(\"learning_rate: \" + str(learning_rate) + \"\\n\")\n",
    "    file.write(\"num_hidden_layers: \" + str(hidden_layers_num) + \"\\n\")\n",
    "    file.write(\"hidden_dim: \" + str(hidden_dim) + \"\\n\")\n",
    "    file.write(\"num_epochs: \" + str(num_epochs) + \"\\n\")\n",
    "    \n",
    "    file.write(\"error(val): \" + str(val_results[0]) + \"\\n\")\n",
    "    file.write(\"accuracy(val):\"  + str(val_results[1])  + \"\\n\")\n",
    "    \n",
    "    for k, v in test_results.items():\n",
    "        file.write(str(k) + \":  \" + str(v) + \"\\n\")\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    file.close()\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_activation(layers, activation, alpha=None, lower=None, upper=None):\n",
    "    if activation == \"LeakyReluLayer\":\n",
    "        layers.append( LeakyReluLayer(alpha=alpha) )\n",
    "    elif activation == \"ParametricReluLayer\":\n",
    "        layers.append( ParametricReluLayer(alpha=alpha) )    \n",
    "    elif activation == \"ExponentialLinearUnitLayer\":\n",
    "        layers.append( ExponentialLinearUnitLayer(alpha=alpha) )    \n",
    "    elif activation == \"RandomReluLayer\":\n",
    "        layers.append( RandomReluLayer(lower=lower, upper=upper) )    \n",
    "        \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "from mlp.data_providers import MNISTDataProvider, EMNISTDataProvider\n",
    "\n",
    "\n",
    "seed = 11102019 \n",
    "rng = np.random.RandomState(seed)\n",
    "batch_size = 100\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "\n",
    "print(\"Training set: \")\n",
    "train_data = EMNISTDataProvider('train', batch_size=batch_size, rng=rng)\n",
    "print(\"Validation set: \")\n",
    "valid_data = EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "print(\"Testing set: \")\n",
    "test_data  = EMNISTDataProvider('test', batch_size=batch_size, rng=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate         = 0.01    \n",
    "num_epochs            = 50\n",
    "stats_interval        = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims           = [128]\n",
    "hidden_dim            = 128\n",
    "hidden_layers_num     = 3  \n",
    "activations           = [\"LeakyReluLayer\", \"RandomReluLayer\", \"ParametricReluLayer\", \"ExponentialLinearUnitLayer\"]\n",
    "optimizer             = \"SGD\"\n",
    "\n",
    "for index in range(1,4):\n",
    "    for activation in activations: \n",
    "        weights_init = GlorotUniformInit(rng=rng)\n",
    "        biases_init  = ConstantInit(0.)\n",
    "\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append( AffineLayer(input_dim, hidden_dim, weights_init, biases_init) )\n",
    "        layers = append_activation(layers, activation) \n",
    "\n",
    "        for i in range(hidden_layers_num):\n",
    "            layers.append( AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init) )\n",
    "            layers = append_activation(layers, activation) \n",
    "\n",
    "\n",
    "        layers.append( AffineLayer(hidden_dim, output_dim, weights_init, biases_init) )\n",
    "\n",
    "\n",
    "        model = MultipleLayerModel( layers )\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "\n",
    "        learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "        \n",
    "\n",
    "        _ = train_model_and_plot_stats(\n",
    "            model, error, learning_rule, learning_rate, train_data, valid_data, hidden_layers_num, hidden_dim, \n",
    "            num_epochs, stats_interval, notebook=True, test_data=test_data, activation=activation, \n",
    "            optimizer=optimizer, index=index)\n",
    "\n",
    "\n",
    "        stats = _[0]\n",
    "        stats_file_name = get_file_name(\"stats/stats\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                        num_epochs, activation=activation, optimizer=optimizer, index=index)\n",
    "        np.savetxt(stats_file_name, stats)\n",
    "\n",
    "\n",
    "        val_results = stats[-1]\n",
    "        val_results = val_results[2:]\n",
    "        test_results = _[-2]\n",
    "        save_results(val_results, test_results, learning_rate, hidden_layers_num, \n",
    "                     hidden_dim, num_epochs, activation=activation, optimizer=optimizer, index=index)\n",
    "\n",
    "\n",
    "        trained_model =  _[-1]\n",
    "        model_params  = trained_model.params\n",
    "        params_file_name = get_file_name(\"model_params/model_params\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                         num_epochs, activation=activation, optimizer=optimizer, \n",
    "                                         index=index, suffix=\".txt\")\n",
    "        save_model_params(model_params, params_file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Leaky RELU alpha values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "learning_rate         = 0.01            \n",
    "num_epochs            = 50\n",
    "stats_interval        = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims           = [128]\n",
    "hidden_dim            = 128\n",
    "hidden_layers_num     = 3  \n",
    "activations           = [\"LeakyReluLayer\", \"RandomReluLayer\", \"ParametricReluLayer\", \"ExponentialLinearUnitLayer\"]\n",
    "activation            = \"LeakyReluLayer\"\n",
    "optimizer             = \"SGD\"\n",
    "alphas                = [0.2, 0.3]\n",
    "\n",
    "for alpha in alphas: \n",
    "    for index in range(1,4):\n",
    "        weights_init = GlorotUniformInit(rng=rng)\n",
    "        biases_init  = ConstantInit(0.)\n",
    "\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        layers.append( AffineLayer(input_dim, hidden_dim, weights_init, biases_init) )\n",
    "        layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "\n",
    "        for i in range(hidden_layers_num):\n",
    "            layers.append( AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init) )\n",
    "            layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "\n",
    "        layers.append( AffineLayer(hidden_dim, output_dim, weights_init, biases_init) )\n",
    "\n",
    "\n",
    "        model = MultipleLayerModel( layers )\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "\n",
    "        #learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "        learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "        _ = train_model_and_plot_stats(\n",
    "            model, error, learning_rule, learning_rate, train_data, valid_data, hidden_layers_num, hidden_dim, \n",
    "            num_epochs, stats_interval, notebook=True, test_data=test_data, activation=activation, \n",
    "            optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "\n",
    "        stats = _[0]\n",
    "        stats_file_name = get_file_name(\"stats/stats\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                        num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "        np.savetxt(stats_file_name, stats)\n",
    "\n",
    "        val_results = stats[-1]\n",
    "        val_results = val_results[2:]\n",
    "        test_results = _[-2]\n",
    "        save_results(val_results, test_results, learning_rate, hidden_layers_num, \n",
    "                     hidden_dim, num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "\n",
    "        trained_model =  _[-1]\n",
    "        model_params  = trained_model.params\n",
    "        params_file_name = get_file_name(\"model_params/model_params\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                         num_epochs, activation=activation, optimizer=optimizer, \n",
    "                                         index=index, alpha=alpha, suffix=\".txt\")\n",
    "        save_model_params(model_params, params_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "learning_rate         = 0.01      # best performing from the baseline experiments            \n",
    "num_epochs            = 50\n",
    "stats_interval        = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims           = [128]\n",
    "hidden_dim            = 128\n",
    "hidden_layers_num     = 3  \n",
    "activations           = [\"LeakyReluLayer\", \"RandomReluLayer\", \"ParametricReluLayer\", \"ExponentialLinearUnitLayer\"]\n",
    "activation            = \"ExponentialLinearUnitLayer\"\n",
    "optimizer             = \"SGD\"\n",
    "alphas                = [0.2]\n",
    "\n",
    "for index in range(1,4):\n",
    "    for alpha in alphas: \n",
    "        weights_init = GlorotUniformInit(rng=rng)\n",
    "        biases_init  = ConstantInit(0.)\n",
    "\n",
    "        # Create the model\n",
    "        layers = []\n",
    "        # Append initial layer + activation\n",
    "        layers.append( AffineLayer(input_dim, hidden_dim, weights_init, biases_init) )\n",
    "        layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "        # Create specified number of hidden layers with appropriate hidden dimension\n",
    "        for i in range(hidden_layers_num):\n",
    "            layers.append( AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init) )\n",
    "            layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "        # Append final layer \n",
    "        layers.append( AffineLayer(hidden_dim, output_dim, weights_init, biases_init) )\n",
    "\n",
    "        # Create the model based on layers\n",
    "        model = MultipleLayerModel( layers )\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "        # Use a basic gradient descent learning rule\n",
    "        #learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "        learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "        \n",
    "        #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "        _ = train_model_and_plot_stats(\n",
    "            model, error, learning_rule, learning_rate, train_data, valid_data, hidden_layers_num, hidden_dim, \n",
    "            num_epochs, stats_interval, notebook=True, test_data=test_data, activation=activation, \n",
    "            optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "        # save all training & validation stats to a file\n",
    "        stats = _[0]\n",
    "        stats_file_name = get_file_name(\"ELU/stats/stats\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                        num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "        np.savetxt(stats_file_name, stats)\n",
    "\n",
    "        # retrieve validation and test results and save them to a file \n",
    "        val_results = stats[-1]\n",
    "        val_results = val_results[2:]\n",
    "        test_results = _[-2]\n",
    "        save_results(val_results, test_results, learning_rate, hidden_layers_num, \n",
    "                     hidden_dim, num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "        # retrieve model parameters and save it to a txt file\n",
    "        trained_model =  _[-1]\n",
    "        model_params  = trained_model.params\n",
    "        params_file_name = get_file_name(\"ELU/model_params/model_params\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                         num_epochs, activation=activation, optimizer=optimizer, \n",
    "                                         index=index, alpha=alpha, suffix=\".txt\")\n",
    "        save_model_params(model_params, params_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric ReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "learning_rate         = 0.01      # best performing from the baseline experiments            \n",
    "num_epochs            = 25\n",
    "stats_interval        = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims           = [128]\n",
    "hidden_dim            = 128\n",
    "hidden_layers_num     = 3  \n",
    "activations           = [\"LeakyReluLayer\", \"RandomReluLayer\", \"ParametricReluLayer\", \"ExponentialLinearUnitLayer\"]\n",
    "activation            = \"ParametricReluLayer\"\n",
    "optimizer             = \"SGD\"\n",
    "alphas                = [0.1, 0.001]\n",
    "\n",
    "for alpha in alphas: \n",
    "    for index in range(1,4):\n",
    "        weights_init = GlorotUniformInit(rng=rng)\n",
    "        biases_init  = ConstantInit(0.)\n",
    "\n",
    "        # Create the model\n",
    "        layers = []\n",
    "        # Append initial layer + activation\n",
    "        layers.append( AffineLayer(input_dim, hidden_dim, weights_init, biases_init) )\n",
    "        layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "        # Create specified number of hidden layers with appropriate hidden dimension\n",
    "        for i in range(hidden_layers_num):\n",
    "            layers.append( AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init) )\n",
    "            layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "        # Append final layer \n",
    "        layers.append( AffineLayer(hidden_dim, output_dim, weights_init, biases_init) )\n",
    "\n",
    "        # Create the model based on layers\n",
    "        model = MultipleLayerModel( layers )\n",
    "\n",
    "        error = CrossEntropySoftmaxError()\n",
    "        # Use a basic gradient descent learning rule\n",
    "        #learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "        learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "        \n",
    "        #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "        _ = train_model_and_plot_stats(\n",
    "            model, error, learning_rule, learning_rate, train_data, valid_data, hidden_layers_num, hidden_dim, \n",
    "            num_epochs, stats_interval, notebook=True, test_data=test_data, activation=activation, \n",
    "            optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "        # save all training & validation stats to a file\n",
    "        stats = _[0]\n",
    "        stats_file_name = get_file_name(\"PReLU/stats/stats\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                        num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "        np.savetxt(stats_file_name, stats)\n",
    "\n",
    "        # retrieve validation and test results and save them to a file \n",
    "        val_results = stats[-1]\n",
    "        val_results = val_results[2:]\n",
    "        test_results = _[-2]\n",
    "        save_results(val_results, test_results, learning_rate, hidden_layers_num, \n",
    "                     hidden_dim, num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "        # retrieve model parameters and save it to a txt file\n",
    "        trained_model =  _[-1]\n",
    "        model_params  = trained_model.params\n",
    "        params_file_name = get_file_name(\"PReLU/model_params/model_params\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                         num_epochs, activation=activation, optimizer=optimizer, \n",
    "                                         index=index, alpha=alpha, suffix=\".txt\")\n",
    "        save_model_params(model_params, params_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Relu bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "learning_rate         = 0.01      # best performing from the baseline experiments            \n",
    "num_epochs            = 50\n",
    "stats_interval        = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims           = [128]\n",
    "hidden_dim            = 128\n",
    "hidden_layers_num     = 3  \n",
    "activations           = [\"LeakyReluLayer\", \"RandomReluLayer\", \"ParametricReluLayer\", \"ExponentialLinearUnitLayer\"]\n",
    "activation            = \"RandomReluLayer\"\n",
    "optimizer             = \"SGD\"\n",
    "alphas                = [0.1, 0.001]\n",
    "lower                 = 0.01\n",
    "upper                 = 0.2\n",
    "\n",
    "for index in range(1,4):\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init  = ConstantInit(0.)\n",
    "\n",
    "    # Create the model\n",
    "    layers = []\n",
    "    # Append initial layer + activation\n",
    "    layers.append( AffineLayer(input_dim, hidden_dim, weights_init, biases_init) )\n",
    "    layers = append_activation(layers, activation, lower=lower, upper=upper) \n",
    "\n",
    "    # Create specified number of hidden layers with appropriate hidden dimension\n",
    "    for i in range(hidden_layers_num):\n",
    "        layers.append( AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init) )\n",
    "        layers = append_activation(layers, activation, lower=lower, upper=upper) \n",
    "\n",
    "    # Append final layer \n",
    "    layers.append( AffineLayer(hidden_dim, output_dim, weights_init, biases_init) )\n",
    "\n",
    "    # Create the model based on layers\n",
    "    model = MultipleLayerModel( layers )\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "\n",
    "    #learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    _ = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, learning_rate, train_data, valid_data, hidden_layers_num, hidden_dim, \n",
    "        num_epochs, stats_interval, notebook=True, test_data=test_data, activation=activation, \n",
    "        optimizer=optimizer, index=index, lower=lower, upper=upper)\n",
    "\n",
    "    # save all training & validation stats to a file\n",
    "    stats = _[0]\n",
    "    stats_file_name = get_file_name(\"RReLU/stats/stats\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                    num_epochs, activation=activation, optimizer=optimizer, index=index,\n",
    "                                    lower=lower, upper=upper)\n",
    "    np.savetxt(stats_file_name, stats)\n",
    "\n",
    "    # retrieve validation and test results and save them to a file \n",
    "    val_results = stats[-1]\n",
    "    val_results = val_results[2:]\n",
    "    test_results = _[-2]\n",
    "    save_results(val_results, test_results, learning_rate, hidden_layers_num, \n",
    "                 hidden_dim, num_epochs, activation=activation, optimizer=optimizer, \n",
    "                 lower=lower, upper=upper, index=index)\n",
    "\n",
    "    # retrieve model parameters and save it to a txt file\n",
    "    trained_model =  _[-1]\n",
    "    model_params  = trained_model.params\n",
    "    params_file_name = get_file_name(\"RReLU/model_params/model_params\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                     num_epochs, activation=activation, optimizer=optimizer, lower=lower, upper=upper,\n",
    "                                     index=index, suffix=\".txt\")\n",
    "    save_model_params(model_params, params_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LReLU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup hyperparameters\n",
    "learning_rate         = 0.01      # best performing from the baseline experiments            \n",
    "num_epochs            = 100\n",
    "stats_interval        = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims           = [128]\n",
    "hidden_dim            = 128\n",
    "hidden_layers_num     = 3  \n",
    "activations           = [\"LeakyReluLayer\", \"RandomReluLayer\", \"ParametricReluLayer\", \"ExponentialLinearUnitLayer\"]\n",
    "activation            = \"LeakyReluLayer\"\n",
    "optimizer             = \"SGD\"\n",
    "alphas                = [0.001, 0.1]\n",
    "alpha                 = 0.01\n",
    "\n",
    "for index in range(1,4):\n",
    "    weights_init = GlorotUniformInit(rng=rng)\n",
    "    biases_init  = ConstantInit(0.)\n",
    "\n",
    "    # Create the model\n",
    "    layers = []\n",
    "    # Append initial layer + activation\n",
    "    layers.append( AffineLayer(input_dim, hidden_dim, weights_init, biases_init) )\n",
    "    layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "    # Create specified number of hidden layers with appropriate hidden dimension\n",
    "    for i in range(hidden_layers_num):\n",
    "        layers.append( AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init) )\n",
    "        layers = append_activation(layers, activation, alpha) \n",
    "\n",
    "    # Append final layer \n",
    "    layers.append( AffineLayer(hidden_dim, output_dim, weights_init, biases_init) )\n",
    "\n",
    "    # Create the model based on layers\n",
    "    model = MultipleLayerModel( layers )\n",
    "\n",
    "    error = CrossEntropySoftmaxError()\n",
    "    # Use a basic gradient descent learning rule\n",
    "    #learning_rule = AdamLearningRule(learning_rate=learning_rate)\n",
    "    learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "    #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "    _ = train_model_and_plot_stats(\n",
    "        model, error, learning_rule, learning_rate, train_data, valid_data, hidden_layers_num, hidden_dim, \n",
    "        num_epochs, stats_interval, notebook=True, test_data=test_data, activation=activation, \n",
    "        optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "    # save all training & validation stats to a file\n",
    "    stats = _[0]\n",
    "    stats_file_name = get_file_name(\"stats/stats\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                    num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "    np.savetxt(stats_file_name, stats)\n",
    "\n",
    "    # retrieve validation and test results and save them to a file \n",
    "    val_results = stats[-1]\n",
    "    val_results = val_results[2:]\n",
    "    test_results = _[-2]\n",
    "    save_results(val_results, test_results, learning_rate, hidden_layers_num, \n",
    "                 hidden_dim, num_epochs, activation=activation, optimizer=optimizer, index=index, alpha=alpha)\n",
    "\n",
    "    # retrieve model parameters and save it to a txt file\n",
    "    trained_model =  _[-1]\n",
    "    model_params  = trained_model.params\n",
    "    params_file_name = get_file_name(\"model_params/model_params\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                     num_epochs, activation=activation, optimizer=optimizer, \n",
    "                                     index=index, alpha=alpha, suffix=\".txt\")\n",
    "    save_model_params(model_params, params_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 2B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup hyperparameters\n",
    "learning_rates        = [0.001, 0.01, 0.1, 0.0001, 1]      \n",
    "num_epochs            = 100\n",
    "stats_interval        = 1\n",
    "input_dim, output_dim = 784, 47\n",
    "hidden_dims           = [32, 64, 128] #, 64, 128\n",
    "hidden_layers_nums    = [1, 2, 3] #, 2, 3  \n",
    "activation            = \"NoActivation\"\n",
    "optimizer             = \"SGD\"\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for hidden_layers_num in hidden_layers_nums: \n",
    "        for hidden_dim in hidden_dims: \n",
    "            for index in range(1,4):        #make multiple experiments\n",
    "                # initialise layers of the model\n",
    "                layers = []\n",
    "\n",
    "                weights_init = GlorotUniformInit(rng=rng)\n",
    "                biases_init  = ConstantInit(0.)\n",
    "\n",
    "                # Append initial layer\n",
    "                layers.append( AffineLayer(input_dim, hidden_dim, weights_init, biases_init) )\n",
    "\n",
    "                # Create specified number of hidden layers with appropriate hidden dimension\n",
    "                for i in range(hidden_layers_num):\n",
    "                    layers.append( AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init) )\n",
    "\n",
    "                # Append final layer \n",
    "                layers.append( AffineLayer(hidden_dim, output_dim, weights_init, biases_init) )\n",
    "\n",
    "                # Create the model based on layers\n",
    "                model = MultipleLayerModel( layers )\n",
    "\n",
    "                error = CrossEntropySoftmaxError()\n",
    "                # Use a basic gradient descent learning rule\n",
    "                learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "                #Remember to use notebook=False when you write a script to be run in a terminal\n",
    "                _ = train_model_and_plot_stats(\n",
    "                    model, error, learning_rule, learning_rate, train_data, valid_data, hidden_layers_num, hidden_dim, \n",
    "                    num_epochs, stats_interval, notebook=True, test_data=test_data, activation=activation, \n",
    "                    optimizer=optimizer, index=index)\n",
    "\n",
    "                # save all training & validation stats to a file\n",
    "                stats = _[0]\n",
    "                stats_file_name = get_file_name(\"no_activation/stats/stats\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                                num_epochs, activation=activation, optimizer=optimizer, index=index)\n",
    "                np.savetxt(stats_file_name, stats)\n",
    "\n",
    "                # retrieve validation and test results and save them to a file \n",
    "                val_results = stats[-1]\n",
    "                val_results = val_results[2:]\n",
    "                test_results = _[-2]\n",
    "                save_results(val_results, test_results, learning_rate, hidden_layers_num, \n",
    "                             hidden_dim, num_epochs, activation=activation, optimizer=optimizer, index=index)\n",
    "\n",
    "                # retrieve model parameters and save it to a txt file\n",
    "                trained_model =  _[-1]\n",
    "                model_params  = trained_model.params\n",
    "                params_file_name = get_file_name(\"no_activation/model_params/model_params\", learning_rate, hidden_layers_num, hidden_dim, \n",
    "                                                 num_epochs, activation=activation, optimizer=optimizer, \n",
    "                                                 index=index, suffix=\".txt\")\n",
    "                save_model_params(model_params, params_file_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
